name: Deploy LabLink Infrastructure

on:
  push:
    branches:
      - test

  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy (test, prod, or ci-test) - dev uses local state and should only be used locally"
        required: true
        default: test
        type: choice
        options:
          - test
          - prod
          - ci-test

  repository_dispatch:
    types: [deploy-prod-image]

permissions:
  id-token: write
  contents: read

jobs:
  terraform:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v3
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.6.6
          terraform_wrapper: false

      - name: Determine Environment
        id: setenv
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            case "${{ github.event.inputs.environment }}" in
              test|prod|ci-test)
                echo "env=${{ github.event.inputs.environment }}" >> "$GITHUB_OUTPUT"
                ;;
              *)
                echo "Unsupported environment: ${{ github.event.inputs.environment }}"
                exit 1
                ;;
            esac
          elif [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            echo "env=${{ github.event.client_payload.environment }}" >> "$GITHUB_OUTPUT"
          elif [ "${{ github.ref_name }}" = "test" ]; then
            echo "env=test" >> "$GITHUB_OUTPUT"
          else
            echo "Unsupported branch or manual input"
            exit 1
          fi

          # Always use lablink-infrastructure directory
          echo "workdir=lablink-infrastructure" >> "$GITHUB_OUTPUT"

      - name: Debug Environment
        run: |
          echo "Using environment: ${{ steps.setenv.outputs.env }}"
          echo "Working directory: ${{ steps.setenv.outputs.workdir }}"

      - name: Log in to GitHub Container Registry
        run: echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Validate configuration before deployment
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: |
          echo "Validating configuration before deployment..."

          CONFIG_FILE="config/config.yaml"

          # Extract allocator image tag from config
          IMAGE_TAG=$(grep -A5 "^allocator:" "$CONFIG_FILE" | grep "image_tag:" | awk '{print $2}' | tr -d '"')

          if [ -z "$IMAGE_TAG" ]; then
            echo "Error: allocator.image_tag not found in $CONFIG_FILE"
            exit 1
          fi

          ALLOCATOR_IMAGE="ghcr.io/talmolab/lablink-allocator-image:${IMAGE_TAG}"
          echo "Using allocator image for validation: $ALLOCATOR_IMAGE"

          # Pull the allocator image
          docker pull "$ALLOCATOR_IMAGE"

          # Create absolute path for config file
          CONFIG_PATH="$(pwd)/config/config.yaml"

          # Run validation using the Docker image
          docker run --rm \
            -v "$CONFIG_PATH:/config/config.yaml:ro" \
            "$ALLOCATOR_IMAGE" \
            uv run lablink-validate-config /config/config.yaml --verbose

          echo "Configuration validation passed!"

      - name: Inject Password Secrets
        working-directory: ${{ steps.setenv.outputs.workdir }}
        env:
          ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD || 'CHANGEME_admin_password' }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD || 'CHANGEME_db_password' }}
        run: |
          CONFIG_FILE="config/config.yaml"

          if [ ! -f "$CONFIG_FILE" ]; then
            echo "Error: $CONFIG_FILE not found"
            exit 1
          fi

          echo "Injecting secrets into $CONFIG_FILE"

          # Replace placeholders with actual values
          sed -i "s/PLACEHOLDER_ADMIN_PASSWORD/${ADMIN_PASSWORD}/g" "$CONFIG_FILE"
          sed -i "s/PLACEHOLDER_DB_PASSWORD/${DB_PASSWORD}/g" "$CONFIG_FILE"

          # Verify no placeholders remain
          if grep -q "PLACEHOLDER_" "$CONFIG_FILE"; then
            echo "ERROR: Failed to replace all placeholders in $CONFIG_FILE"
            exit 1
          fi

          # Warn if using default passwords
          if [[ "$ADMIN_PASSWORD" == "CHANGEME_"* ]] || [[ "$DB_PASSWORD" == "CHANGEME_"* ]]; then
            echo "::warning::Using default passwords! Set ADMIN_PASSWORD and DB_PASSWORD secrets for security."
          fi

          echo "Passwords injected successfully"

      - name: Terraform Init
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: ../scripts/init-terraform.sh ${{ steps.setenv.outputs.env }}

      - name: Terraform Format
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: terraform fmt -check

      - name: Terraform Validate
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: terraform validate

      - name: Import Existing CloudWatch Log Groups
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: |
          # Try to import existing CloudWatch log groups if they exist
          # This prevents ResourceAlreadyExistsException errors from partial/failed deploys
          echo "Attempting to import existing CloudWatch log groups..."

          # Import client VM log group (ignore error if doesn't exist)
          terraform import aws_cloudwatch_log_group.client_vm_logs \
            "lablink-cloud-init-${{ steps.setenv.outputs.env }}" 2>/dev/null || \
            echo "  Client VM log group doesn't exist yet (will be created)"

          # Import Lambda log group (ignore error if doesn't exist)
          terraform import aws_cloudwatch_log_group.lambda_logs \
            "/aws/lambda/lablink_log_processor_${{ steps.setenv.outputs.env }}" 2>/dev/null || \
            echo "  Lambda log group doesn't exist yet (will be created)"

          echo "Import attempt completed"

      - name: Terraform Plan
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: |
          terraform plan \
            -var="resource_suffix=${{ steps.setenv.outputs.env }}"

      - name: Terraform Apply
        id: apply
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: |
          terraform apply -auto-approve \
            -var="resource_suffix=${{ steps.setenv.outputs.env }}"
        continue-on-error: true

      - name: Save PEM Key to Artifact
        if: steps.apply.outcome == 'success'
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: |
          terraform output -raw private_key_pem > lablink-key.pem
          chmod 600 lablink-key.pem
          mkdir -p artifact-output
          mv lablink-key.pem artifact-output/

      - name: Upload PEM Artifact
        if: steps.apply.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: lablink-key-${{ steps.setenv.outputs.env }}
          path: ${{ steps.setenv.outputs.workdir }}/artifact-output/lablink-key.pem
          retention-days: 1

      - name: Verify Deployment
        if: steps.apply.outcome == 'success'
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: |
          ../scripts/verify-deployment.sh --ci ${{ steps.setenv.outputs.env }}

      - name: Deployment Summary
        if: steps.apply.outcome == 'success'
        working-directory: ${{ steps.setenv.outputs.workdir }}
        run: |
          echo "Deployment succeeded!"
          echo "Environment: ${{ steps.setenv.outputs.env }}"

          echo ""
          echo "Allocator FQDN:"
          terraform output -raw allocator_fqdn

          echo ""
          echo "EC2 Key Name:"
          terraform output -raw ec2_key_name

          echo ""
          echo "EC2 Public IP:"
          terraform output -raw ec2_public_ip

      - name: Terraform Destroy on Failure
        working-directory: ${{ steps.setenv.outputs.workdir }}
        if: steps.apply.outcome == 'failure'
        run: terraform destroy -auto-approve -var="resource_suffix=${{ steps.setenv.outputs.env }}"
